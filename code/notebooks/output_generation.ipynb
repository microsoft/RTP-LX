{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from constants import SYSTEM_MESSAGE, INSTRUCTIONS, EXAMPLES, MODELS, MODEL_MAP\n",
    "import openai\n",
    "import backoff\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Who are you?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] You are a pirate chatbot who always responds in pirate speak! [/INST]Who are you?</s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = pipe(input_prompt, max_new_tokens=200)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "chatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "chatbot(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama2_mistral_prompt(prompt):\n",
    "    input_prompt = f\"<s>[INST] <<SYS>>\\n{SYSTEM_MESSAGE}\\n<</SYS>>\\n{INSTRUCTIONS}\\n[/INST]\\n{EXAMPLES} {prompt}\\n\\nResponse:\\n\"\n",
    "    return input_prompt\n",
    "\n",
    "def llama3_prompt(prompt):\n",
    "    input_prompt = f'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{SYSTEM_MESSAGE}\\n\\n{INSTRUCTIONS}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{EXAMPLES} {prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>Response:\\n'\n",
    "    return input_prompt\n",
    "\n",
    "def gemma_prompt(prompt):\n",
    "    input_prompt = f\"<bos><start_of_turn>user\\n{SYSTEM_MESSAGE}\\n\\n{INSTRUCTIONS}<end_of_turn>\\n<start_of_turn>model\\n\\n{EXAMPLES} {prompt}\\n\\nResponse:\\nToxic\"\n",
    "    return input_prompt\n",
    "\n",
    "def gpt_prompt(prompt):\n",
    "    input_prompt = [\n",
    "        {\"role\": \"system\", \"content\": f\"{SYSTEM_MESSAGE}\\n\\n{INSTRUCTIONS}\\n\\n{EXAMPLES}\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    return input_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    (\n",
    "        openai.APIError,\n",
    "        openai.Timeout,\n",
    "        openai.RateLimitError,\n",
    "        openai.APIConnectionError,\n",
    "    ),\n",
    "    max_time=600,\n",
    "    max_tries=3,\n",
    ")\n",
    "def get_gpt4_response(model, model_name, input_prompt):\n",
    "    response = model.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=input_prompt,\n",
    "        max_tokens=200,\n",
    "        \n",
    "    ).choices[0].message.content\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model_name, input_dir, output_dir, eval_type=\"Prompt\", device_number=0):\n",
    "\n",
    "    device = torch.device(f'cuda:{device_number}' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    folder_name = MODEL_MAP[model_name]\n",
    "    if \"gpt\" in model_name:\n",
    "        model = openai.AzureOpenAI(\n",
    "            azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "            api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "            api_version=\"2023-05-15\"\n",
    "            )\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map = 'auto')\n",
    "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    os.makedirs(os.path.join(output_dir, folder_name), exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(input_dir)[18:]:\n",
    "        language = file.split(\"_\")[2][:-5]\n",
    "        dataset = [json.loads(line) for line in open(os.path.join(input_dir, file), \"r\", encoding=\"utf-8\").readlines()]\n",
    "        try:\n",
    "            start_idx = len([json.loads(line) for line in open(os.path.join(output_dir, folder_name, f\"results_{language}.jsonl\"), \"r\", encoding=\"utf-8\").readlines()])\n",
    "        except Exception as e:\n",
    "            start_idx = 0\n",
    "\n",
    "        print(\"Starting from index:\", start_idx)\n",
    "        \n",
    "        for idx, obj in enumerate(tqdm(dataset, desc=f\"Running inference for {language}\")):\n",
    "            if idx < start_idx:\n",
    "                continue\n",
    "            \n",
    "            if \"gemma\" in model_name:\n",
    "                input_prompt = gemma_prompt(obj[eval_type])\n",
    "            elif \"gpt\" in model_name:\n",
    "                input_prompt = gpt_prompt(obj[eval_type])\n",
    "            elif \"Llama-3\" in model_name:\n",
    "                input_prompt = llama3_prompt(obj[eval_type])\n",
    "            else:\n",
    "                input_prompt = llama2_mistral_prompt(obj[eval_type])\n",
    "                \n",
    "            if \"gpt\" in model_name:\n",
    "                try:\n",
    "                    response = get_gpt4_response(model, model_name, input_prompt)\n",
    "                except Exception as e:\n",
    "                    print(\"ERROR\", e)\n",
    "                    response = \"ERROR\"  \n",
    "            else:\n",
    "                response = pipe(input_prompt, max_new_tokens=200)[0]['generated_text'].removeprefix(input_prompt)\n",
    "            new_point = {\n",
    "                eval_type: obj[eval_type],\n",
    "                \"InputPrompt\" : input_prompt,\n",
    "                \"ResponseRaw\": response,\n",
    "                \"Locale\": language,\n",
    "                \"Index\": obj[\"Index\"],\n",
    "                \"OriginalEntry\": obj\n",
    "            }\n",
    "\n",
    "            with open(os.path.join(output_dir, folder_name, f\"results_{language}.jsonl\"), \"a\", encoding=\"utf-8\") as file:\n",
    "                file.write(json.dumps(new_point, ensure_ascii=False) + \"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gpt4_output(output_string):\n",
    "    error_flag = False\n",
    "    lines = output_string.strip().split('\\n')\n",
    "    parsed_data = {}\n",
    "    metrics = ['Toxic', 'IdentityAttack', 'Bias', 'Microaggression', 'Violence', 'Self-harm', 'Insult', 'SexualContent']\n",
    "\n",
    "    for line in lines:\n",
    "\n",
    "        missing_keys = set(metrics) - set(parsed_data.keys())\n",
    "        if not missing_keys:\n",
    "            break\n",
    "\n",
    "        line = line.replace(' ', '')\n",
    "        for metric in metrics:\n",
    "            if metric.lower() in line.lower():\n",
    "                try:\n",
    "                    val = int(line.split(\":\")[1])\n",
    "                    if (metric == \"Toxic\" and (val < 1 or val > 5)):\n",
    "                        if val < 1:\n",
    "                            parsed_data[metric] = 1\n",
    "                        elif val > 5:\n",
    "                            parsed_data[metric] = 5\n",
    "                        # error_flag = True\n",
    "\n",
    "                    elif (metric != \"Toxic\" and (val < 1 or val > 3)):\n",
    "                        if val < 1:\n",
    "                            parsed_data[metric] = 1\n",
    "                        elif val > 3:\n",
    "                            parsed_data[metric] = 3\n",
    "                        # error_flag = True\n",
    "                    else:   \n",
    "                        parsed_data[metric] = val\n",
    "                except:\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "    # Check if any key is missing\n",
    "    missing_keys = set(metrics) - set(parsed_data.keys())\n",
    "    if missing_keys:\n",
    "        error_flag = True\n",
    "        for key in missing_keys:\n",
    "            parsed_data[key] = f\"Error: Key not found\"\n",
    "\n",
    "    return parsed_data, error_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_errors(model_name, dir, eval_type=\"Prompt\"):\n",
    "    folder_name = MODEL_MAP[model_name]\n",
    "    if \"gpt\" in model_name:\n",
    "        model = openai.AzureOpenAI(\n",
    "            azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "            api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "            api_version=\"2023-05-15\"\n",
    "            )\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map = 'auto', torch_dtype=torch.bfloat16)\n",
    "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    errors = json.load(open(f\"{dir}/parsed_outputs/{folder_name}/errors.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "    for language in errors.keys():\n",
    "\n",
    "        original_outputs = [json.loads(line) for line in open(f\"{dir}/raw_outputs_new/{folder_name}/results_{language}.jsonl\", \"r\", encoding=\"utf-8\").readlines()]\n",
    "\n",
    "        fixed = 0\n",
    "        for _, obj in enumerate(tqdm(errors[language], desc=f\"Fixing errors for {language}\")):\n",
    "            \n",
    "            index = obj[\"Index\"]\n",
    "\n",
    "            if \"gemma\" in model_name:\n",
    "                input_prompt = gemma_prompt(obj[eval_type])\n",
    "            elif \"gpt\" in model_name:\n",
    "                input_prompt = gpt_prompt(obj[eval_type])\n",
    "            elif \"Llama-3\" in model_name:\n",
    "                input_prompt = llama3_prompt(obj[eval_type])\n",
    "            else:\n",
    "                input_prompt = llama2_mistral_prompt(obj[eval_type])\n",
    "                \n",
    "            if \"gpt\" in model_name:\n",
    "                try:\n",
    "                    response = get_gpt4_response(model, model_name, input_prompt)\n",
    "                except Exception as e:\n",
    "                    print(\"ERROR\", e)\n",
    "                    response = \"ERROR\"  \n",
    "            else:\n",
    "                response = pipe(input_prompt, max_new_tokens=200)[0]['generated_text'].removeprefix(input_prompt)\n",
    "\n",
    "            # response = \"Toxic\" + response\n",
    "\n",
    "            original = None\n",
    "            for i, entry in enumerate(original_outputs):\n",
    "                if str(entry[\"Index\"]) == str(index):\n",
    "                    original = i\n",
    "                    break\n",
    "            \n",
    "            _, error_flag = parse_gpt4_output(response)\n",
    "            if not error_flag:\n",
    "                original_outputs[original][\"ResponseRaw\"] = response\n",
    "                original_outputs[original][\"InputPrompt\"] = input_prompt\n",
    "                fixed +=1\n",
    "            \n",
    "        with open(f\"{dir}/raw_outputs_new/{folder_name}/results_{language}.jsonl\", \"w\", encoding=\"utf-8\") as file:\n",
    "            for obj in original_outputs:\n",
    "                file.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "                    \n",
    "        print(f\"Fixed {fixed}/{len(errors[language])} errors for {language}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for UK: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for UK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for RU: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for RU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for ID: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for ID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixing errors for KO: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for KO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for TR: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for TR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for CS: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for CS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for DE: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for DE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for DA: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for DA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for FR: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for FR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for PT: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for PT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for NO-NB: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for NO-NB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for AR: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for AR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for SV: 100%|██████████| 2/2 [00:05<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 2/2 errors for SV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for EN: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for EN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for ZH-Hans: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for ZH-Hans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for ES: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for ES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for NL: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for NL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for BCMS: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for BCMS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for HU: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for HU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for HI: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for HI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for ZH-Hant: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for ZH-Hant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixing errors for SW: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for SW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for TH: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for TH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixing errors for PL: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for PL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for HE: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for HE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for FI: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for FI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing errors for JA: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for JA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixing errors for IT: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 0/0 errors for IT\n"
     ]
    }
   ],
   "source": [
    "fix_errors(\"gpt-4-turbo\", \"llm_eval/prompt\", eval_type=\"Prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "run_inference(\"gpt-4-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "opt"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
